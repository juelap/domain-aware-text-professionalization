{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for the baseline predictions (corpus) and the one we want to compare (specific dataset) \n",
    "\n",
    "columns = [\"Source\", \"Target\", \"Prediction\"]\n",
    "dataset = \"Sports\"\n",
    "\n",
    "path_specific_model = \"5_predictions/multi_label_true/\"+dataset+\"/predictions_\"+dataset+\"_dataset_with_\"+dataset+\"_model.csv\"\n",
    "path_full_model = \"5_predictions/multi_label_true/\"+dataset+\"/predictions_\"+dataset+\"_dataset_with_All_model.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions into Dataframes\n",
    "\n",
    "df_prediction_specific = pd.read_csv(path_specific_model, sep=',', header=None, names=columns)\n",
    "df_prediction_full = pd.read_csv(path_full_model, sep=',', header=None, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1167, 3)\n",
      "(1167, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_prediction_specific.shape)\n",
    "print(df_prediction_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE metrics\n",
    "\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "rouge_scores_specific = rouge.get_scores(df_prediction_specific[\"Prediction\"], df_prediction_specific[\"Target\"], avg=True)\n",
    "rouge_scores_full = rouge.get_scores(df_prediction_full[\"Prediction\"], df_prediction_full[\"Target\"], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ROUGE scores for Sports dataset with Sports model. \n",
      "\n",
      "{'rouge-1': {'f': 0.51617681826468,\n",
      "             'p': 0.4064753462666721,\n",
      "             'r': 0.7577614476222978},\n",
      " 'rouge-2': {'f': 0.33486253611473193,\n",
      "             'p': 0.2527580630777826,\n",
      "             'r': 0.5592082049728957},\n",
      " 'rouge-l': {'f': 0.5015817835285318,\n",
      "             'p': 0.39494530810842793,\n",
      "             'r': 0.7365048981024421}}\n",
      "\n",
      "\n",
      "ROUGE scores for Sports dataset with All model. \n",
      "\n",
      "{'rouge-1': {'f': 0.5796625756783801,\n",
      "             'p': 0.4637787273137406,\n",
      "             'r': 0.8211833459237012},\n",
      " 'rouge-2': {'f': 0.42592453198753416,\n",
      "             'p': 0.3260831841898893,\n",
      "             'r': 0.6883455632667943},\n",
      " 'rouge-l': {'f': 0.5718549968595438,\n",
      "             'p': 0.45763805858991374,\n",
      "             'r': 0.8097400571958293}}\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n\\nROUGE scores for {dataset} dataset with {dataset} model. \\n')\n",
    "pprint.pprint(rouge_scores_specific)\n",
    "\n",
    "print(f'\\n\\nROUGE scores for {dataset} dataset with All model. \\n')\n",
    "pprint.pprint(rouge_scores_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU metrics\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(predictions, targets):\n",
    "    score_bleu1 = 0.\n",
    "    score_bleu2 = 0.\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        score_bleu1 += sentence_bleu([targets[i].split()], predictions[i].split(), weights=(1, 0, 0, 0))\n",
    "    score_bleu1 /= len(predictions)\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        score_bleu2 += sentence_bleu([targets[i].split()], predictions[i].split(), weights=(0, 1, 0, 0))\n",
    "    score_bleu2 /= len(predictions)\n",
    "    \n",
    "    return [score_bleu1, score_bleu2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu_scores_specific = calculate_bleu(df_prediction_specific[\"Prediction\"], df_prediction_specific[\"Target\"])\n",
    "bleu_scores_full = calculate_bleu(df_prediction_full[\"Prediction\"], df_prediction_full[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\BLEU scores for Sports dataset with Sports model. \n",
      "\n",
      "[0.3228586900523488, 0.23442490817235054]\n",
      "\n",
      "\\BLEU scores for Sports dataset with All model. \n",
      "\n",
      "[0.3643774389837662, 0.2983391891572673]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n\\BLEU scores for {dataset} dataset with {dataset} model. \\n')\n",
    "pprint.pprint(bleu_scores_specific)\n",
    "\n",
    "print(f'\\n\\BLEU scores for {dataset} dataset with All model. \\n')\n",
    "pprint.pprint(bleu_scores_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# Calculate BERTScore metrics\n",
    "\n",
    "from evaluate import load\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "model = \"microsoft/deberta-xlarge-mnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore for Sports dataset with Sports model. \n",
      "\n",
      "F1: 0.7121308270998254\n",
      "Precision: 0.6302744090786936\n",
      "Recall: 0.8267768141338737\n"
     ]
    }
   ],
   "source": [
    "predictions_specific = df_prediction_specific[\"Prediction\"]\n",
    "references_specific = df_prediction_specific[\"Target\"]\n",
    "results_specific = bertscore.compute(predictions=predictions_specific, references=references_specific, lang=\"en\", model_type=model)\n",
    "#pprint.pprint(results_specific)\n",
    "f1_score_specific = mean(results_specific[\"f1\"])\n",
    "precision_specific = mean(results_specific[\"precision\"])\n",
    "recall_specific = mean(results_specific[\"recall\"])\n",
    "\n",
    "print(f'BERTScore for {dataset} dataset with {dataset} model. \\n')\n",
    "print(f'F1: {f1_score_specific}')\n",
    "print(f'Precision: {precision_specific}')\n",
    "print(f'Recall: {recall_specific}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore for Sports dataset with All model. \n",
      "\n",
      "F1: 0.695192341632892\n",
      "Precision: 0.5887148081745566\n",
      "Recall: 0.8705070673551126\n"
     ]
    }
   ],
   "source": [
    "predictions_full = df_prediction_full[\"Prediction\"]\n",
    "references_full = df_prediction_full[\"Target\"]\n",
    "results_full = bertscore.compute(predictions=predictions_full, references=references_full, lang=\"en\", model_type=model)\n",
    "#pprint.pprint(results_full)\n",
    "f1_score_specific = mean(results_full[\"f1\"])\n",
    "precision_specific = mean(results_full[\"precision\"])\n",
    "recall_specific = mean(results_full[\"recall\"])\n",
    "\n",
    "print(f'BERTScore for {dataset} dataset with All model. \\n')\n",
    "print(f'F1: {f1_score_specific}')\n",
    "print(f'Precision: {precision_specific}')\n",
    "print(f'Recall: {recall_specific}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
